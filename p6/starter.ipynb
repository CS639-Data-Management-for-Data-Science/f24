{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88073,"databundleVersionId":10087583,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# P6 Starter - Time Series Analysis \n\n### Statistical Modeling to Deep Learning","metadata":{}},{"cell_type":"markdown","source":"##  Imports & Sanity Check (Do NOT Change)","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\nfrom tqdm.notebook import tqdm\nimport statsmodels.api as sm # PACF, ACF\nfrom typing import Tuple, List\n\n# Viz:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option(\"future.no_silent_downcasting\", True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Utilities. Read the Fn names atleast so that you are not re-writing code\n\n* **make_submission**: Helps you convert your predictions to competition submission ready files.\n* **rmsle**: Implementation of the metric used to evaluate your score on the leaderboard.\n* **lgbm_rmsle**: Definition that can be used to do train-val type training while printing metric scores.","metadata":{}},{"cell_type":"code","source":"def make_submission(test_preds):\n    \"\"\"\n    Args:\n        test_preds: Your predictions from the model.\n\n    NOTE: Your test_predictions should be in the same order as the test set. \n          This function does not take care of unsorted/shuffled predictions.\n    \"\"\"\n    test = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/test.csv\")\n    submission_df = pd.DataFrame(columns = [\"id\", \"sales\"])\n    submission_df.sales = test_preds\n    submission_df.id = test.id.astype(int)\n    submission_df.to_csv(\"submission.csv\", index = False)\n\n\ndef rmsle(y_pred, y_true):\n    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n\n\ndef lgbm_rmsle(preds, train_data):\n    labels = train_data.get_label()\n    rmsle_val = rmsle(preds, labels)\n    return 'RMSLE', rmsle_val, False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the data (Do NOT Change)","metadata":{}},{"cell_type":"code","source":"#########################\n# DO NOT CHANGE\n#########################\n\ntrain = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/test.csv\")\nstores = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/stores.csv\")\n#sub = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/sample_submission.csv\")   \ntransactions = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/transactions.csv\").sort_values([\"store_nbr\", \"date\"])\noil = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/oil.csv\")\nholidays = pd.read_csv(\"/kaggle/input/cs-639-p-5-time-series-forecasting/store-sales-time-series-forecasting/holidays_events.csv\")\n\n# Datetime\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\ntransactions[\"date\"] = pd.to_datetime(transactions.date)\noil[\"date\"] = pd.to_datetime(oil.date)\nholidays[\"date\"] = pd.to_datetime(holidays.date)\n\n# Data types\ntrain.onpromotion = train.onpromotion.astype(\"float16\")\ntrain.sales = train.sales.astype(\"float32\")\nstores.cluster = stores.cluster.astype(\"int8\")\n\n# Process holidays and events\ntr1 = holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr2 = holidays[(holidays.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr = pd.concat([tr1,tr2], axis = 1)\ntr = tr.iloc[:, [5,1,2,3,4]]\n\nholidays = holidays[(holidays.transferred == False) & (holidays.type != \"Transfer\")].drop(\"transferred\", axis = 1)\nholidays = holidays._append(tr).reset_index(drop = True)\n\n# Additional Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n\n# Bridge Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n \n# Work Day Holidays, that is meant to payback the Bridge.\nwork_day = holidays[holidays.type == \"Work Day\"]  \nholidays = holidays[holidays.type != \"Work Day\"]  \n\n# Events are national\nevents = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\nholidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis = 1)\nregional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\nnational = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\nlocal = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\nevents[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n\ntrain.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 1: EDA & Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Q1 Left join transaction to train and then print the Spearman Correlation between Total Sales and Transactions.","metadata":{}},{"cell_type":"code","source":"# TODO - q1\nprint(\"Spearman Correlation between Total Sales and Transactions:\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q2 Plot an 'ordinary least squares' trendline between transactions and sales to verify the spearman correlation value in Q1. [0.1 Points]","metadata":{}},{"cell_type":"code","source":"# TODO - q2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q3 Plot these line charts in the notebook:\n\nA) Transactions vs Date (all stores color coded in the same plot) \n\r\nB) Average monthly transactions\n\n C) Average Transactions on the days of the wee)\n","metadata":{}},{"cell_type":"code","source":"# TODO - q3 - Plot A","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO - q3 - Plot B","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO - q3 - Plot C","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q4 Use pandas' in-build (linear) interpolation to impute the missing oil values then overlay the imputed feature over the original.\n\nYour new feature column should be called: `dcoilwtico_interpolated`","metadata":{}},{"cell_type":"code","source":"# Interpolate. \n\n# Plot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q5 Again, left join oil on the dataframe above and report the spearman correlation between oil and sales and oil and transactions","metadata":{}},{"cell_type":"code","source":"print(\"Correlation with Daily Oil Prices:\")\n# Find correlation with sales & transactions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q6 Report the top-3 highest negative correlations between oil and sales of a particular product family. Now think whether oil should be discarded as a feature?\n\nUse sort_values","metadata":{}},{"cell_type":"code","source":"# Calculate all correlations\n\n# Report the top 3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q7. Which 2 features do you think fit the description of look-ahead data leakage? [0.1 Points]","metadata":{}},{"cell_type":"code","source":"print(\"The 2 data leakage features are: <feature-1> and <feature-2>\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q8. One hot encode the holidays and events data and similarly left join it to the the main dataframe.\n\nYou just have to finish the one-hot encoder function definition for this one.","metadata":{}},{"cell_type":"code","source":"def one_hot_encoder(df, nan_as_category=True) -> Tuple[pd.DataFrame, List[str]]:\n    # One hot encoding (pandas can do it on 1 line!) \n    \n    # Store the new columns in a list\n    \n    # Replace \" \" with \"_\" in column names.\n    \n    # Return the new dataframe and all the columns (as a list)\n    pass","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#########################\n# DO NOT CHANGE. \n# NOTE: Run this after you have implemented the one_hot_encoder function above.\n#########################\n\nd = pd.merge(train._append(test), stores)\nd[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n\n# National Holidays & Events\nd = pd.merge(d, national, how = \"left\")\n# Regional\nd = pd.merge(d, regional, how = \"left\", on = [\"date\", \"state\"])\n# Local\nd = pd.merge(d, local, how = \"left\", on = [\"date\", \"city\"])\n\n# Work Day: It will be removed when real work day colum created\nd = pd.merge(d,  work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis = 1),how = \"left\")\n\nevents, events_cat = one_hot_encoder(events, nan_as_category=False)\nevents[\"events_Dia_de_la_Madre\"] = np.where(events.date == \"2016-05-08\", 1, events[\"events_Dia_de_la_Madre\"])\nevents = events.drop(239)\n\nd = pd.merge(d, events, how = \"left\")\nd[events_cat] = d[events_cat].fillna(0)\n\n# New features\nd[\"holiday_national_binary\"] = np.where(d.holiday_national.notnull(), 1, 0)\nd[\"holiday_local_binary\"] = np.where(d.holiday_local.notnull(), 1, 0)\nd[\"holiday_regional_binary\"] = np.where(d.holiday_regional.notnull(), 1, 0)\nd[\"national_independence\"] = np.where(d.holiday_national.isin(['Batalla de Pichincha',  'Independencia de Cuenca', 'Independencia de Guayaquil', 'Independencia de Guayaquil', 'Primer Grito de Independencia']), 1, 0)\nd[\"local_cantonizacio\"] = np.where(d.holiday_local.str.contains(\"Cantonizacio\"), 1, 0)\nd[\"local_fundacion\"] = np.where(d.holiday_local.str.contains(\"Fundacion\"), 1, 0)\nd[\"local_independencia\"] = np.where(d.holiday_local.str.contains(\"Independencia\"), 1, 0)\n\n\nholidays, holidays_cat = one_hot_encoder(d[[\"holiday_national\",\"holiday_regional\",\"holiday_local\"]], nan_as_category=False)\nd = pd.concat([d.drop([\"holiday_national\",\"holiday_regional\",\"holiday_local\"], axis = 1),holidays], axis = 1)\n\nhe_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\nd[he_cols] = d[he_cols].astype(\"int8\")\n\nd[[\"family\", \"city\", \"state\", \"type\"]] = d[[\"family\", \"city\", \"state\", \"type\"]].astype(\"category\")\n\ndel holidays, holidays_cat, work_day, local, regional, national, events, events_cat, tr, tr1, tr2, he_cols\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# d.tail(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#########################\n# DO NOT CHANGE\n#########################\ntrain = d[d.date < \"2017-08-01\"]\ntest = d[d.date >= \"2017-08-01\"]\ntest = test.drop([\"sales\"], axis=1)\n\ntrain = pd.get_dummies(train, columns=train.select_dtypes(['object']).columns)\ntrain = pd.get_dummies(train, columns=train.select_dtypes(['category']).columns)\nfor col in train.columns:\n    if pd.api.types.is_numeric_dtype(train[col]):\n        train[col] = train[col].astype('float32')\n\ntest = pd.get_dummies(test, columns=test.select_dtypes(['object']).columns)\ntest = pd.get_dummies(test, columns=test.select_dtypes(['category']).columns)\nfor col in test.columns:\n    if pd.api.types.is_numeric_dtype(test[col]):\n        test[col] = test[col].astype('float32')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"replace_dict = {}\nfor x in train.cols:\n    if \"family\" in x:\n        if \" \" in x:\n            lst = x.split(\" \")\n            new_feature_name = \"\"\n            for k in lst:\n                new_feature_name += str(k)\n                new_feature_name += \"_\"\n            replace_dict[x] = new_feature_name[:-1]\n        if \",\" in x: \n            lst = x.split(\",\")\n            new_feature_name = \"\"\n            for k in lst:\n                new_feature_name += str(k)\n                new_feature_name += \"_\"\n            replace_dict[x] = new_feature_name[:-1]\n        if \"/\" in x:\n            lst = x.split(\",\")\n            new_feature_name = \"\"\n            for k in lst:\n                new_feature_name += str(k)\n                new_feature_name += \"_\"\n            replace_dict[x] = new_feature_name[:-1]\n        \n    elif \"state\" in x:\n        lst = x.split(\",\")\n        new_feature_name = \"\"\n        for k in lst:\n            new_feature_name += str(k)\n            new_feature_name += \"_\"\n        replace_dict[x] = new_feature_name[:-1]\n\nreplace_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.rename(columns=replace_dict)\ntest.rename(columns=replace_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\ntrain = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntest = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 2","metadata":{}},{"cell_type":"markdown","source":"### Q9. EMA\n\nForecast window should be >=15 days since the test set is 15 days. **For this question use 16 as the forecast window**","metadata":{}},{"cell_type":"code","source":"# Train EMAs for each family per store (pandas has an inbuilt ema function!)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make the predictions\n\n# Use the make_submission utility function provided to save a submission CSV. \n\n# Submit to competition and note your RMSLE score somewhere for this model type.\n\n# NOTE - 1: You still need to go on the right panel and click submit \n# (make_submission will NOT submit to competition -> It just makes a submission ready file)\n# NOTE - 2: Ensure that you are not overwriting your submission.csv file in subsequent cells.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q10. Linear Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Create a dictionary to store all the models (1 model per family per store)\nmodels = {}\n# 2. Fit these models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Look through each test record and find the corresponding model in models\n\n# 2. Make the predictions on the test set.\n\n# 3. Make the submission using the utility function `make_submission` provided \n\n# 4. Submit to competition and note your RMSLE score somewhere for this model type.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q11. PACF and ACF\n\nUse lib sm \n\n(statsmodel.api is already imported as sm)","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.graphics.tsaplots import plot_acf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Group by date and family\n\n# 2. Plot 33x2 charts (ACF & PACF for each family)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q12. ADF Test -> ARIMA","metadata":{}},{"cell_type":"code","source":"# 1. Groupby date, sales and onpromotion\n\n# 2. Now get the sales time series\n\n# 3. Find the autocorrelation of the time series you just store in step 2.\n\n# Print autocorrelation\nprint(\"Autocorrelation:\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot ACF on sales time series","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the PACF\nfig, ax = plt.subplots(figsize=(10, 6))\n\n##########\n# TODO: Your plot code goes here:\n##########\n\n##########\nplt.xlabel('Lag')\nplt.ylabel('Partial Autocorrelation')\nplt.title('Partial Autocorrelation Function (PACF)')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Differencing technique\r\nThis process is meant to transform the time series data to stationary, as ARIMA model only works with stationary time series data.","metadata":{}},{"cell_type":"code","source":"# 1. Compute and store the diff series\n\n# 2. Drop NA or any other erroneous values.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute the autocorrelation (nlags = 20)\n\n# Plot the autocorrelation chart (use plt.stem)\nplt.figure(figsize=(10, 6))\n\n############\n# TODO: Your code goes here:\n############\n\n############\n\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.title('Autocorrelation Chart')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Augmented Dickey-Fuller (ADF) test\n\r\nThe Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or non-stationary. Stationarity is an important assumption in many time series analysis models.\r\n\r\nThe ADF test evaluates the null hypothesis that the time series has a unit root, indicating non-stationarity. The alternative hypothesis is that the time series is stationary.\r\n\r\nWhen performing the ADF test, we obtain the ADF statistic and the p-value. The ADF statistic is a negative number and the more negative it is, the stronger the evidence against the null hypothesis. The p-value represents the probability of observing the ADF statistic or a more extreme value if the null hypothesis were true. A low p-value (below a chosen significance level, typically 0.05) indicates strong evidence against the null hypothesis and suggests that the time series is stationary.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO - q12 \n# 1. Perform the ADF test\n\n# 2. Extract the test statistics and p-value\n\n# 3. Print these values\nprint(\"ADF Statistic:\")\nprint(\"p-value:\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The ADF statistic is (around) -11.4. This statistic is a negative value and is more negative than the critical values at common significance levels. This suggests strong evidence against the null hypothesis of a unit root, indicating that the time series is stationary.\r\n\r\nThe p-values (around)  i6.76e-2121, which is a very small value close to zero. Typically, if the p-value is below a chosen significance level (e.g., 0.05), it indicates strong evidence to reject the null hypothesis. In your case, the extremely small p-value suggests strong evidence against the presence of a unit root and supports the stationarity of the time series.","metadata":{}},{"cell_type":"markdown","source":"**TODO** Choose the right p, q and d values for your ARIMA model","metadata":{}},{"cell_type":"code","source":"# TODO: Replace with appropriate p,d,q values for ARIMA\np_arima = None\n\nd_arima = None\n\nq_arima = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Get sales series as training data (np array with appropriate dtype)\n\n# 2. Using statsmodel.tsa lib. Initialize an ARIMA model with the p,d,q params you defined. \n\n# 3. Fit the model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the post model fitting summary\n# print(result.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions & submit to competition using your best model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Section 3","metadata":{}},{"cell_type":"markdown","source":"### Q13 Define a validation set. What will be the most appropriate time period for this validation set?","metadata":{}},{"cell_type":"code","source":"# Get the val set:","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q14. LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process your data to the appropriate dtypes, vars, etc.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the lgb.Dataset method to intialize your dataset iterables.\n\n# 1. Make one for the train set:\n\n# 2. Make another for the val set you defined in Q13:\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill the dict with appropriate params:\nlgb_params = {'num_leaves': ,\n              'learning_rate': ,\n              'feature_fraction': ,\n              'max_depth': ,\n              'verbose': 20,\n              'num_boost_round': ,\n              'early_stopping_rounds': ,\n              'nthread': -1}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Complete the model initialization/train params)\nmodel = lgb.train(lgb_params, ... ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Predict the sales value on your val set using the best_iteration recorded by the LGBM\n# 2. Compute and print the RMSLE on this val set.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Pre-process your test set to appropriate format.\n# 2. Predict -> Save using make_submission -> Submit to competition\n# 3. Note your RMSLE for LGBM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q15. CatBoost","metadata":{}},{"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill out missing params for catboost appropriately here:\ncatboost_params = {\n    'iterations': ,           # Number of boosting rounds\n    'learning_rate': ,        # Learning rate for gradient boosting\n    'depth': ,                   # Depth of each tree\n    'loss_function': 'RMSLE',      # Loss function (Root Mean Squared Error for regression)\n    'eval_metric': 'RMSLE',        # Evaluation metric\n    'random_seed': 42,            # Ensures reproducibility\n    'early_stopping_rounds': ,  # Stops training if no improvement after 50 rounds\n    'verbose': 100                # Prints training progress every 100 rounds\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Define the model\n\n# 2. Fit\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Preprocess your test data appropriately\n\n# 4. Make Predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Use make_submission -> Submit to competition\n\n# 6. Note your RMSLE for this model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q16. XGBoost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Initialize model with random state = 42 to be consistent with CatBoost\n\n# 2. Fit\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Make Predictions.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. make_submission -> Submit to competition \n\n# 5. Note your RMSLE ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q17. Optuna for automatic hyperparameter optimization","metadata":{}},{"cell_type":"code","source":"import optuna\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective_lgb(trial):\n    # 1. Define the parameter search space\n    \n    # 2. Create datasets (train, val) for LightGBM\n\n    # 3. Train the model\n\n    # 4. Evaluate on the validation set\n\n    # 5. Return the metric score\n    \n    pass\n\n# Create Optuna study to minimize the objective function\n\nstart = time.time()\n# 1. Create the optuna study and specify appropriate direction\n\n# 2. Optimize (pay attention to recommended trials; 50 takes too long)\n\n# 3. Get the best parameters\n\n# 4. Print them.\n# print(\"Best parameters:\", best_params)\n\nprint(\"Took:\", time.time() - start, \"seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make a competition submission using these parameters\n# Note these values.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do the same for Catboost","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Do the same for XGBoost","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q18. Which out of the three Catboost vs LightGBM vs XGBoost provides the best score? Why do you think this model is more suited to this dataset/problem?","metadata":{}},{"cell_type":"code","source":"print(\"<Your answer goes here>\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optional Extra Credit Section - Achieve the lowest score\n\n### Cross Validation Strategies & Ensembling","metadata":{}},{"cell_type":"code","source":"# 1. Try different Validation sets \n# 2. Try ensembling different methods used in this assignment together","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
